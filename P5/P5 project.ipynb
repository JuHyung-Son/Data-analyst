{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Select features\n",
    "\n",
    "    import sys\n",
    "    import pickle\n",
    "    sys.path.append(\"../tools/\")\n",
    "    import matplotlib.pyplot\n",
    "    from feature_format import featureFormat, targetFeatureSplit\n",
    "    from tester import dump_classifier_and_data\n",
    "    from sklearn.pipeline import Pipeline\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "    features_list = ['poi','salary','bonus','long_term_incentive','total_payments','total_stock_value',\n",
    "                     'deferral_payments','deferred_income','director_fees',\n",
    "                     'exercised_stock_options','expenses','from_messages','from_poi_to_this_person',\n",
    "                     'from_this_person_to_poi','loan_advances','long_term_incentive','other',\n",
    "                     'restricted_stock','restricted_stock_deferred','shared_receipt_with_poi','to_messages'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "    with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "        data_dict = pickle.load(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2\n",
    "## outlier removing\n",
    "\n",
    "\n",
    "features= ['poi','salary','bonus','long_term_incentive','total_payments','total_stock_value',\n",
    "                 'deferral_payments','deferred_income','director_fees',\n",
    "                 'exercised_stock_options','expenses','from_messages','from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi','loan_advances','long_term_incentive','other',\n",
    "                 'restricted_stock','restricted_stock_deferred','shared_receipt_with_poi','to_messages'] # You will need to use more features\n",
    "\n",
    "\n",
    "    data = featureFormat(data_dict, features)\n",
    "\n",
    "    for point in data:\n",
    "        salary = point[1]\n",
    "        bonus = point[2]\n",
    "        matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "    matplotlib.pyplot.xlabel(\"salary\")\n",
    "    matplotlib.pyplot.ylabel(\"bonus\")\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i]['salary'] != 'NaN' and data_dict[i]['salary'] > 25000000:\n",
    "            print i\n",
    "      \n",
    "the outlier is 'TOTAL' and this is not a person, so i don't need it.\n",
    "\n",
    "Deleting outlier \n",
    "\n",
    "    data_dict.pop('TOTAL')\n",
    "\n",
    "see whether it works well\n",
    "\n",
    "    data = featureFormat(data_dict, features)\n",
    "\n",
    "    for point in data:\n",
    "        salary = point[1]\n",
    "        bonus = point[2]\n",
    "        matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "    matplotlib.pyplot.xlabel(\"salary\")\n",
    "    matplotlib.pyplot.ylabel(\"bonus\")\n",
    "    matplotlib.pyplot.show()\n",
    " \n",
    "search for another outlier\n",
    "\n",
    "    for point in data:\n",
    "        salary = point[1]\n",
    "        total_payments = point[4]\n",
    "        matplotlib.pyplot.scatter( salary, total_payments )\n",
    "\n",
    "    matplotlib.pyplot.xlabel(\"salary\")\n",
    "    matplotlib.pyplot.ylabel(\"total_payments\")\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i]['total_payments'] != 'NaN' and data_dict[i]['total_payments'] > 100000000:\n",
    "            print i\n",
    "the outlier is LAY KENNETH L, so this outlier is important information.\n",
    "\n",
    "    for point in data:\n",
    "        salary = point[1]\n",
    "        loan_advances = point[14]\n",
    "        matplotlib.pyplot.scatter( salary, loan_advances )\n",
    "\n",
    "    matplotlib.pyplot.xlabel(\"salary\")\n",
    "    matplotlib.pyplot.ylabel(\"loan_advances\")\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i]['loan_advances'] != 'NaN' and data_dict[i]['loan_advances'] > 7000000:\n",
    "            print i\n",
    "\n",
    "Again the outlier is LAY KENNETH L.\n",
    "So i decide to removie only TOTAL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "## Create new feature\n",
    "\n",
    "I create 'ratio_from_poi_to_this_person' this shows how many emails they send each other.\n",
    "This is pretty good information to find poi.\n",
    "\n",
    "    my_dataset = data_dict\n",
    "    for i in my_dataset:\n",
    "        if my_dataset[i]['from_poi_to_this_person'] != 'NaN' and\\\n",
    "        my_dataset[i]['from_this_person_to_poi'] != 'NaN' and\\\n",
    "        my_dataset[i]['to_messages'] != 'NaN' and\\\n",
    "        my_dataset[i]['from_messages'] != 'NaN':\n",
    "            my_dataset[i]['ratio_from_poi_to_entire_email'] = (my_dataset[i]['from_poi_to_this_person']+my_dataset[i]['from_this_person_to_poi'])/float((my_dataset[i]['from_messages']+my_dataset[i]['to_messages']))\n",
    "        else:\n",
    "            my_dataset[i]['ratio_from_poi_to_entire_email'] = 0\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "# GaussianNB\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(features,labels)\n",
    "    pred = clf.predict(features)\n",
    "    precision = precision_score(labels,pred)\n",
    "    recall = recall_score(labels,pred)\n",
    "    f1 = f1_score(labels,pred)\n",
    "    print precision\n",
    "    print recall\n",
    "    print f1\n",
    "\n",
    "### DecisionTreeClassifer\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(features,labels)\n",
    "    pred = clf.predict(features)\n",
    "    test_classifier(clf, data_dict, features_list)\n",
    "\n",
    "### AdaBoost\n",
    "    clf = AdaBoostClassifier()\n",
    "    clf.fit(features,labels)\n",
    "    pred = clf.predict(features)\n",
    "    test_classifier(clf, data_dict, features_list)\n",
    "\n",
    "### KMeans\n",
    "    clf = KMeans(n_clusters = 2)\n",
    "    clf.fit(features,labels)\n",
    "    pred = clf.predict(features)\n",
    "    test_classifier(clf, data_dict, features_list)\n",
    "    \n",
    "    \n",
    "My resuly shows AdaBoost is best classifier yet.\n",
    "But interesting thing is this Adaboost shows f1 score higherthan 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.grid_search import GridSearchCV\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "### split data\n",
    "\n",
    "To cross validate, I use StatifiedShuffleSplit. Because this code isn't used in the class. And this is good for small dataset. Because 'This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.'\n",
    "'http://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html#sklearn.cross_validation.StratifiedShuffleSplit'\n",
    "\n",
    "    from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(labels, 10,test_size = 0.4, random_state = 90)\n",
    "\n",
    "    for sss_train, sss_test in sss:\n",
    "        features_train = []\n",
    "        features_test = []\n",
    "        labels_train = []\n",
    "        labels_test = []\n",
    "        #creates train,test data\n",
    "        for ii in sss_train:\n",
    "            features_train.append(features[ii])\n",
    "            labels_train.append(labels[ii])\n",
    "        for jj in sss_test:\n",
    "            features_test.append(features[jj])\n",
    "            labels_test.append(labels[jj])\n",
    "\n",
    "\n",
    "### Tuning classifier\n",
    "\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    kbest = SelectKBest()\n",
    "    scaler = MinMaxScaler()\n",
    "    ada = AdaBoostClassifier()\n",
    "    cv =StratifiedShuffleSplit(labels_train, 90,test_size = 0.3,random_state=50)\n",
    "    pca = PCA()\n",
    "\n",
    "    parameters = {'kbest__k':[15,16,17,18,19,20],'scaler__copy':[True, False]}\n",
    "    pipe = Pipeline([('scaler',scaler),('kbest',kbest),('ada',ada)])\n",
    "    grid_search = GridSearchCV(pipe,parameters, cv=cv, scoring = 'f1')\n",
    "    clf =grid_search.fit(features_train,labels_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    test_classifier(clf, data_dict, features_list)\n",
    "\n",
    "\n",
    "    Pipeline(steps=[('scaler', MinMaxScaler(copy=False, feature_range=(0, 1))), ('kbest', SelectKBest(k=20, score_func=<function f_classif at 0x000000000D9D7B38>)), ('ada', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
    "              learning_rate=1.0, n_estimators=50, random_state=None))])\n",
    "            Accuracy: 0.84653       Precision: 0.40026      Recall: 0.30300 F1: 0.34491     F2: 0.31848\n",
    "            Total predictions: 15000        True positives:  606    False positives:  908   False negatives: 1394   True negatives: 12092\n",
    "        \n",
    "parameters is 'k' in SelectKBest, k is number of features and i don't know how many features is good for learning, so i put 1~20 features, and result shows 20 features is best and 'n_estimators' in AdaBoost which decide maximum number of estimator.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I use pipeline with SelectKBest, MinMaxScaler, AdaBoost.\n",
    "To avoid overfitting, I use StratifiedShuffleSplit as a cross validation in pipeline.\n",
    "To standadize, i use MinMaxScaler\n",
    "To feature select, i use selectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "    test_classifier(clf, data_dict, features_list)\n",
    "    dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
